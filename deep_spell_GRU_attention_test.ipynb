{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_spell_GRU_attention_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ono97F_Xi8BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfHDzpuE-2ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import json\n",
        "import glob\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import os,sys\n",
        "import random\n",
        "import string\n",
        "import pickle\n",
        "import logging\n",
        "import itertools\n",
        "import unicodedata\n",
        "import torch.nn as nn\n",
        "from fastai.imports import *\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from collections import Counter\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from __future__ import print_function, division, unicode_literals\n",
        "from numpy.random import choice as random_choice,randint as random_randint, shuffle as random_shuffle, seed as random_seed, rand\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FaHJ3jCsjUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run\n",
        "# Set a logger for the module\n",
        "LOGGER = logging.getLogger(__name__) # Every log will use the module name\n",
        "LOGGER.addHandler(logging.StreamHandler())\n",
        "LOGGER.setLevel(logging.DEBUG)\n",
        "random_seed(123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckxiOUEwsjiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run\n",
        "class Configuration(object):\n",
        "    \"\"\"Dump stuff here\"\"\"\n",
        "\n",
        "CONFIG = Configuration()\n",
        "#pylint:disable=attribute-defined-outside-init\n",
        "# Parameters for the model:\n",
        "CONFIG.input_layers = 2\n",
        "CONFIG.output_layers = 2\n",
        "CONFIG.amount_of_dropout = 0.2\n",
        "CONFIG.hidden_size = 500\n",
        "CONFIG.number_of_chars = 100\n",
        "CONFIG.max_input_len = 60\n",
        "CONFIG.inverted = False\n",
        "\n",
        "# Parameters for the dataset\n",
        "MIN_INPUT_LEN = 5\n",
        "AMOUNT_OF_NOISE = 0.2 / CONFIG.max_input_len\n",
        "CHARS = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .\")\n",
        "PADDING = \"☕\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzSb3wilsjnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RUN ONCE\n",
        "#file_list = glob.glob(\"/content/drive/My Drive/1-billion/training/*.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJPGQ-MHsjvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.listdir(\"/content/drive/My Drive/1-billion/heldout\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE6AIABqnF1d",
        "colab_type": "text"
      },
      "source": [
        "Run only once\n",
        "> \n",
        "Reads all the txt files in variable file_list to result.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6jNJUItsj5d",
        "colab_type": "code",
        "outputId": "9bcea741-2b1f-4e9c-bf7e-eb248052ab42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''with open(\"/content/drive/My Drive/1-billion/result.txt\", \"wb\") as outfile:\n",
        "    for f in file_list:\n",
        "        with open(f, \"rb\") as infile:\n",
        "            outfile.write(infile.read())'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'with open(\"/content/drive/My Drive/1-billion/result.txt\", \"wb\") as outfile: #ONLY ONCE\\n    for f in file_list:\\n        with open(f, \"rb\") as infile:\\n            outfile.write(infile.read())'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6eqr89ksjzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run\n",
        "DATA_FILES_FULL_PATH = '/content/drive/My Drive/1-billion/'\n",
        "#NEWS_FILE_NAME = 'del_v2.txt'\n",
        "NEWS_FILE_NAME = '/content/drive/My Drive/1-billion/result.txt'\n",
        "NEWS_FILE_NAME_CLEAN = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.clean\")\n",
        "NEWS_FILE_NAME_FILTERED = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.filtered\")\n",
        "NEWS_FILE_NAME_SPLIT = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.split\")\n",
        "NEWS_FILE_NAME_TRAIN = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.train\")\n",
        "NEWS_FILE_NAME_VALIDATE = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.validate\")\n",
        "NEWS_FILE_NAME_TEST = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.test\")\n",
        "\n",
        "NEWS_FILE_NAME_TRAIN_20 = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.train_20\")\n",
        "NEWS_FILE_NAME_TRAIN_40 = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.train_40\")\n",
        "NEWS_FILE_NAME_TRAIN_60 = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.train_60\")\n",
        "NEWS_FILE_NAME_TRAIN_80 = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.train_80\")\n",
        "NEWS_FILE_NAME_TRAIN_90 = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.train_90\")\n",
        "NEWS_FILE_NAME_TRAIN_100 = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.train_100\")\n",
        "\n",
        "#NEWS_FILE_NAME_VALIDATE_20 = os.path.join(DATA_FILES_FULL_PATH, \"news.2013.en.validate_20\")\n",
        "CHAR_FREQUENCY_FILE_NAME = os.path.join(DATA_FILES_FULL_PATH, \"char_frequency.json\")\n",
        "SAVED_MODEL_FILE_NAME = os.path.join(DATA_FILES_FULL_PATH, \"keras_spell_e{}.h5\") # an HDF5 file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0DeuNhdsjq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_letters = all_letters = string.ascii_letters + \" .,);'(-\"\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def cleaned_text(text):\n",
        "    result = re.compile(r'[^\\S\\n]+', re.UNICODE).sub(' ', text.strip())\n",
        "    \n",
        "    result = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE).sub('-', result)\n",
        "    \n",
        "    result = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(chr(768), chr(769), chr(832), chr(833), chr(2387), chr(5151),\n",
        "                                                                                      chr(5152), chr(65344), chr(8242)), re.UNICODE).sub(\"'\", result)\n",
        "    result = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE).sub(\"(\", result)\n",
        "    \n",
        "    result = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE).sub(\")\", result)\n",
        "    \n",
        "    result = re.compile(r'[^\\w\\s{}{}]'.format(re.escape(\"\"\"¥£₪$€฿₨\"\"\"), re.escape(\"\"\"-!?/;\"'%&<>.()[]{}@#:,|=*\"\"\")), re.UNICODE).sub('', result)\n",
        "\n",
        "    result = unicodeToAscii(result)\n",
        "    result = normalizeString(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def preprocesses_data_clean():\n",
        "    with open(NEWS_FILE_NAME_CLEAN, \"w\") as clean_data:\n",
        "        for line in open(NEWS_FILE_NAME, 'r', encoding=\"utf-8\"):\n",
        "            cleaned_line = cleaned_text(line)\n",
        "            clean_data.write(cleaned_line + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1GhhwQln563",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7EmAItGsjly",
        "colab_type": "code",
        "outputId": "9d01616c-8108-445c-99ce-8fa86fed218d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from __future__ import print_function, division, unicode_literals            # RUN ONCE\\npreprocesses_data_clean()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3jqD3wp8D84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RUN ONCE\n",
        "def preprocesses_data_analyze_chars():\n",
        "    \"\"\"Pre-process the data - step 2 - analyze the characters\"\"\"\n",
        "    counter = Counter()\n",
        "    LOGGER.info(\"Reading data:\")\n",
        "    for line in open(NEWS_FILE_NAME_CLEAN,  'r', encoding=\"utf-8\"):\n",
        "        decoded_line = line#.decode('utf-8')\n",
        "        counter.update(decoded_line)\n",
        "    LOGGER.info(\"Done.\\nWriting to file:\")\n",
        "    with open(CHAR_FREQUENCY_FILE_NAME, 'w', encoding=\"utf-8\") as output_file:\n",
        "        output_file.write(json.dumps(counter))\n",
        "    most_popular_chars = {key for key, _value in counter.most_common(CONFIG.number_of_chars)}\n",
        "    LOGGER.info(\"The top %s chars are:\", CONFIG.number_of_chars)\n",
        "    LOGGER.info(\"\".join(sorted(most_popular_chars)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0qySmrk8EAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RUN ONCE\n",
        "def read_top_chars():\n",
        "    \"\"\"Read the top chars we saved to file\"\"\"\n",
        "    chars = json.loads(open(CHAR_FREQUENCY_FILE_NAME).read())\n",
        "    counter = Counter(chars)\n",
        "    most_popular_chars = {key for key, _value in counter.most_common(CONFIG.number_of_chars)}\n",
        "    return most_popular_chars\n",
        "\n",
        "def preprocesses_data_filter():\n",
        "    \"\"\"Pre-process the data - step 3 - filter only sentences with the right chars\"\"\"\n",
        "    most_popular_chars = read_top_chars()\n",
        "    LOGGER.info(\"Reading and filtering data:\")\n",
        "    with open(NEWS_FILE_NAME_FILTERED, \"w\") as output_file:\n",
        "        for line in open(NEWS_FILE_NAME_CLEAN, 'r', encoding='utf-8'):\n",
        "            decoded_line = line#.decode('utf-8')\n",
        "            if decoded_line and not bool(set(decoded_line) - most_popular_chars):\n",
        "                output_file.write(line)\n",
        "    LOGGER.info(\"Done.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ly_cJOc8EI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RUN ONCE\n",
        "def read_filtered_data():\n",
        "    \"\"\"Read the filtered data corpus\"\"\"\n",
        "    LOGGER.info(\"Reading filtered data:\")\n",
        "    lines = open(NEWS_FILE_NAME_FILTERED).read().decode('utf-8').split(\"\\n\")\n",
        "    LOGGER.info(\"Read filtered data - %s lines\", len(lines))\n",
        "    return lines\n",
        "\n",
        "def preprocesses_split_lines():\n",
        "    LOGGER.info(\"Reading filtered data:\")\n",
        "    answers = set()\n",
        "    with open(NEWS_FILE_NAME_SPLIT, \"wb\") as output_file:\n",
        "        for line in open(NEWS_FILE_NAME_FILTERED,'r'):\n",
        "            #line = _line.decode('utf-8')\n",
        "            while len(line) > MIN_INPUT_LEN:\n",
        "                if len(line) <= CONFIG.max_input_len:\n",
        "                    answer = line\n",
        "                    line = \"\"\n",
        "                else:\n",
        "                    space_location = line.rfind(\" \", MIN_INPUT_LEN, CONFIG.max_input_len - 1)\n",
        "                    if space_location > -1:\n",
        "                        answer = line[:space_location]\n",
        "                        line = line[len(answer) + 1:]\n",
        "                    else:\n",
        "                        space_location = line.rfind(\" \") # no limits this time\n",
        "                        if space_location == -1:\n",
        "                            break # we are done with this line\n",
        "                        else:\n",
        "                            line = line[space_location + 1:]\n",
        "                            continue\n",
        "                answers.add(answer)\n",
        "                output_file.write(answer.encode('utf-8') + b\"\\n\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QeXl7MNpaXC",
        "colab_type": "text"
      },
      "source": [
        "Processing the whole data was consuming up almost all the available RAM on colab (~25 GB). So i split the data into multiple parts and processesed the separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3V029rI8EQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RUN ONCE\n",
        "def preprocess_partition_data():\n",
        "    \"\"\"Set asside data for validation\"\"\"\n",
        "    answers = open(NEWS_FILE_NAME_SPLIT).read().split(\"\\n\")\n",
        "    print('shuffle', end=\" \")\n",
        "    random_shuffle(answers)\n",
        "\n",
        "    split_at = len(answers) // 5\n",
        "\n",
        "    with open(NEWS_FILE_NAME_TRAIN_20, \"wb\") as output_file:\n",
        "        output_file.write(\"\\n\".join(answers[:split_at]).encode('utf-8'))\n",
        "\n",
        "    with open(NEWS_FILE_NAME_TRAIN_40, \"wb\") as output_file:\n",
        "        output_file.write(\"\\n\".join(answers[split_at:2*split_at]).encode('utf-8'))\n",
        "\n",
        "    with open(NEWS_FILE_NAME_TRAIN_60, \"wb\") as output_file:\n",
        "        output_file.write(\"\\n\".join(answers[2*split_at:3*split_at]).encode('utf-8'))\n",
        "\n",
        "    with open(NEWS_FILE_NAME_TRAIN_80, \"wb\") as output_file:\n",
        "        output_file.write(\"\\n\".join(answers[3*split_at:4*split_at]).encode('utf-8'))\n",
        "\n",
        "    with open(NEWS_FILE_NAME_TRAIN_90, \"wb\") as output_file:\n",
        "        output_file.write(\"\\n\".join(answers[4*split_at:math.floor(4.5*split_at)]).encode('utf-8'))\n",
        "\n",
        "    with open(NEWS_FILE_NAME_VALIDATE, \"wb\") as output_file:\n",
        "        output_file.write(\"\\n\".join(answers[math.ceil(4.5*split_at):]).encode('utf-8'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJoUtmbq8EVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run\n",
        "def add_noise_to_string(a_string, amount_of_noise):\n",
        "    \"\"\"Add some artificial spelling mistakes to the string\"\"\"\n",
        "    if rand() < amount_of_noise * len(a_string):\n",
        "        # Replace a character with a random character\n",
        "        random_char_position = random_randint(len(a_string))\n",
        "        a_string = a_string[:random_char_position] + random_choice(CHARS[:-1]) + a_string[random_char_position + 1:]\n",
        "    if rand() < amount_of_noise * len(a_string):\n",
        "        # Delete a character\n",
        "        random_char_position = random_randint(len(a_string))\n",
        "        a_string = a_string[:random_char_position] + a_string[random_char_position + 1:]\n",
        "    if len(a_string) < CONFIG.max_input_len and rand() < amount_of_noise * len(a_string):\n",
        "        # Add a random character\n",
        "        random_char_position = random_randint(len(a_string))\n",
        "        a_string = a_string[:random_char_position] + random_choice(CHARS[:-1]) + a_string[random_char_position:]\n",
        "    if rand() < amount_of_noise * len(a_string):\n",
        "        # Transpose 2 characters\n",
        "        random_char_position = random_randint(len(a_string) - 1)\n",
        "        a_string = (a_string[:random_char_position] + a_string[random_char_position + 1] + a_string[random_char_position] +\n",
        "                    a_string[random_char_position + 2:])\n",
        "    return a_string\n",
        "\n",
        "\n",
        "def generate_question(answer):\n",
        "    \"\"\"Generate a question by adding noise\"\"\"\n",
        "    question = add_noise_to_string(answer, AMOUNT_OF_NOISE)\n",
        "    return question, answer\n",
        "\n",
        "\n",
        "def generate_news_data():\n",
        "    \"\"\"Generate some news data\"\"\"\n",
        "    print (\"Generating Data\")\n",
        "    with open(NEWS_FILE_NAME_VALIDATE,'r') as f:\n",
        "      answers = f.read().split(\"\\n\")\n",
        "    print('shuffle', end=\" \")\n",
        "    random_shuffle(answers)\n",
        "    print(\"Done\")\n",
        "    pairs = []\n",
        "    lolz = len(answers)\n",
        "    nn=0\n",
        "\n",
        "    start = time.time()\n",
        " \n",
        "    for answer in answers:\n",
        "      if answer != \"\":\n",
        "        question = add_noise_to_string(answer, AMOUNT_OF_NOISE)\n",
        "        nn+=1\n",
        "        if random_randint(10000) == 8: # Show some progress\n",
        "            print (lolz)\n",
        "            print(nn)\n",
        "        pairs.append((question,answer))\n",
        "    end = time.time()\n",
        "    print(end - start) \n",
        "    tuple_pairs = tuple(pairs)\n",
        "    pairs = None\n",
        "    del pairs, answers\n",
        "    gc.collect()\n",
        "    return tuple_pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeVHCKwm62TU",
        "colab_type": "text"
      },
      "source": [
        "Uncomment and Run the following cell just once. this will do all the preprocessing and save preprocessed files in the drive. Hence the user won't have to carry out preprocessing every time he/she tries running the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTk7wngu4MTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocesses_data_clean()\n",
        "#preprocesses_data_analyze_chars()\n",
        "#preprocesses_data_filter()\n",
        "#preprocesses_split_lines()\n",
        "#preprocess_partition_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8qmrXsp8Ef1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = generate_news_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-CWxkhN8Ec8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run everything after this\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "all_letters = string.ascii_letters + \" .,);'(-\"\n",
        "alpha2index = {'☕': 0, \"SOS\":SOS_token, \"EOS\":EOS_token}\n",
        "index2alpha = {PAD_token: \"☕\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "for num,let in enumerate(all_letters):\n",
        "    alpha2index[let] = num+3\n",
        "    index2alpha[num+3] = let\n",
        "num_words = len(alpha2index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_roxsgT8Ea4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binaryMatrix(L, value=alpha2index[PADDING]):\n",
        "    m =[]\n",
        "    for i,seq in enumerate(L):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token==alpha2index[PADDING]:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "def indexesFromSentence(sentence):\n",
        "    return [alpha2index[alpha] for alpha in sentence] + [EOS_token]\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def inputVar(L):\n",
        "    indexes_batch = [indexesFromSentence(sentt) for sentt in L]\n",
        "    lengths = torch.tensor([len(index) for index in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "def outputVar(L):\n",
        "    indexes_batch = [indexesFromSentence(sentt) for sentt in L]\n",
        "    max_target_len = max([len(index) for index in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "    \n",
        "        \n",
        "def batch2TrainData(pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    input_batch, output_batch = [],[]\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch)\n",
        "    output, mask, max_target_len = outputVar(output_batch)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "small_batch_size = 3\n",
        "batches = batch2TrainData([random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I6OdC6K8EYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n",
        "        \n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        outputs,hidden = self.gru(packed, hidden)\n",
        "        outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        return outputs, hidden\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3DpyXHrtvFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66_1COvuB96z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Luong attention layer\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBJZpJ9H8ET4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers, dropout):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "        \n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout=dropout\n",
        "        \n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout, bidirectional=False)\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, input_seq, hidden, encoder_outputs):\n",
        "        #input_seq = input_seq.view(1,-1)\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        rnn_output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aojp-CPO9Gup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TTu1KTb9HA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, \n",
        "          decoder_optimizer, batch_size, clip, max_length=CONFIG.max_input_len):\n",
        "    \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    \n",
        "    #Setting device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    mask = mask.to(device)\n",
        "    \n",
        "    #Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "    \n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "    #print(\"ENCODER Input - \", input_variable)\n",
        "\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "    \n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "    \n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    temp_list = [] \n",
        "\n",
        "    nqueries=0                                #F\n",
        "    total_fscore = 0                          #F\n",
        "    total_accuracy = 0\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_input = target_variable[t].view(1,-1)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item()*nTotal)\n",
        "            n_totals += nTotal\n",
        "            assert math.isnan(mask_loss)==False,print(\"mask Loss -\",mask_loss, \"decoder Output - \", decoder_output, \"target_variable[t] -\", target_variable[t])\n",
        "            \n",
        "            _, topi = decoder_output.topk(1) \n",
        "            y_true =  target_variable[t].cpu()\n",
        "            y_pred = torch.squeeze(topi).cpu()\n",
        "            fscore = f1_score(y_true, y_pred, average='micro')            \n",
        "            total_fscore += fscore\n",
        "\n",
        "            accuracy = accuracy_score(y_true, y_pred)\n",
        "            total_accuracy += accuracy \n",
        "            nqueries +=1\n",
        "            \n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)#########Check\n",
        "            _, topi = decoder_output.topk(1) \n",
        "            temp_list.append(topi)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item()*nTotal)\n",
        "            n_totals+=nTotal\n",
        "\n",
        "            \n",
        "            y_true =  target_variable[t].cpu()\n",
        "            y_pred = torch.squeeze(topi).cpu()\n",
        "            fscore = f1_score(y_true, y_pred, average='micro')        \n",
        "            total_fscore += fscore\n",
        "\n",
        "            accuracy = accuracy_score(y_true, y_pred)\n",
        "            total_accuracy += accuracy \n",
        "            nqueries +=1\n",
        "\n",
        "    F1_score = total_fscore/nqueries \n",
        "    acc_score = total_accuracy/nqueries \n",
        "\n",
        "    loss.backward()\n",
        "    \n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "    \n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    return sum(print_losses)/n_totals, F1_score,acc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Tcp9f_9Hch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(model_name, pairs, encoder,decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers,\n",
        "               decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "    \n",
        "    training_batches = [batch2TrainData([random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
        "    print(\"initializing...\")\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "\n",
        "    net_f1score = 0                         #F\n",
        "    f1batch_count = 0                         #F\n",
        "\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration']+1\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration,n_iteration+1):\n",
        "        training_batch = training_batches[iteration-1]\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "        loss, F1_score, acc_score = train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, \n",
        "                     embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss+= loss\n",
        "\n",
        "        if random.random()>0.9999:\n",
        "          net_f1score = 0                         #F\n",
        "          f1batch_count = 0 \n",
        "          print_loss = 0\n",
        "          print('---------------------------------------------------zeroing---------------------------------------------------')\n",
        "\n",
        "        net_f1score += F1_score                         #F\n",
        "        f1batch_count += 1                         #F\n",
        "        F1_net = net_f1score/f1batch_count\n",
        "        print_loss_avg = print_loss/f1batch_count\n",
        "\n",
        "        if iteration % print_every == 0:\n",
        "            print(\"Iteration : {};    % complete : {:0.1f}%;           Average Loss : {:.4f}%;            Avg Accuracy: {:.3f}%; Inst_accuracy : {:.3f}%\".format(iteration, iteration/n_iteration*100, print_loss_avg*100, F1_net*100, acc_score*100))\n",
        "            #print_loss=0\n",
        "            \n",
        "        if iteration % save_every == 0:\n",
        "            directory = Path(save_dir + model_name + '/' + corpus_name + '/' + \"{}-{}_{}\".format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                torch.save({\n",
        "                    'iteration' : iteration,\n",
        "                    'en' : encoder.state_dict(),\n",
        "                    'de' : decoder.state_dict(),\n",
        "                    'en_opt' : encoder_optimizer.state_dict(),\n",
        "                    'de_opt' : decoder_optimizer.state_dict(),\n",
        "                    'loss' : loss,\n",
        "                    'embedding' : embedding.state_dict()\n",
        "                },'/content/drive/My Drive/1-billion/deep_spell_attn/'+ '{}_{}.tar'.format(iteration, 'checkpoint'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enos1kM69HfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        \n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long)*SOS_token\n",
        "        \n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        \n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            \n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            \n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "            \n",
        "        return all_tokens, all_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCLmgRPF9Ho7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, searcher, sentence, max_length=CONFIG.max_input_len):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [index2alpha[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            input_sentence = cleaned_text(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'PAD')] #x == 'EOS' or \n",
        "            print('Bot:', ''.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ33WhBH9Hw_",
        "colab_type": "code",
        "outputId": "b42d81aa-0554-476f-ae2a-48ddf4755c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model_name = 'seq2seq_model'\n",
        "attn_model = 'dot'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.2\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = \"/content/drive/My Drive/1-billion/deep_spell_attn/240000_checkpoint.tar\"\n",
        "checkpoint_iter = 4000\n",
        "\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, num_words, decoder_n_layers, dropout)\n",
        "\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvG_0Y6u9H2-",
        "colab_type": "code",
        "outputId": "01c37de1-0607-4d16-9290-02b5a52984f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "'''save_dir = \"/content/drive/My Drive/1-billion/deep_spell_attn/\"\n",
        "corpus_name=\"model\"\n",
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 300000\n",
        "print_every = 10\n",
        "save_every = 2500\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have cuda, configure cuda to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'save_dir = \"/content/drive/My Drive/1-billion/deep_spell_attn/\"\\ncorpus_name=\"model\"\\n# Configure training/optimization\\nclip = 50.0\\nteacher_forcing_ratio = 1\\nlearning_rate = 0.0001\\ndecoder_learning_ratio = 5.0\\nn_iteration = 300000\\nprint_every = 10\\nsave_every = 2500\\n\\n# Ensure dropout layers are in train mode\\nencoder.train()\\ndecoder.train()\\n\\n# Initialize optimizers\\nprint(\\'Building optimizers ...\\')\\nencoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\\nif loadFilename:\\n    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\\n    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\\n\\n# If you have cuda, configure cuda to call\\nfor state in encoder_optimizer.state.values():\\n    for k, v in state.items():\\n        if isinstance(v, torch.Tensor):\\n            state[k] = v.cuda()\\n\\nfor state in decoder_optimizer.state.values():\\n    for k, v in state.items():\\n        if isinstance(v, torch.Tensor):\\n            state[k] = v.cuda()\\n\\n# Run training iterations\\nprint(\"Starting Training!\")\\ntrainIters(model_name, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\\n           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\\n           print_every, save_every, clip, corpus_name, loadFilename)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWb8BTAV9H7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC2l_TbR5JFJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cAzEz7uB5FVG",
        "colab": {}
      },
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        \n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long)*SOS_token\n",
        "        \n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        \n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            \n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            \n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "            \n",
        "        return all_tokens, all_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7L6XhexH5FVW",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, searcher, sentence, max_length=CONFIG.max_input_len):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [index2alpha[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, pairr):\n",
        "    # Get input sentence\n",
        "    input_sentence = pairr[0]\n",
        "    # Evaluate sentence\n",
        "    output_words = evaluate(encoder, decoder, searcher, input_sentence)\n",
        "    # Format and print response sentence\n",
        "    output_words[:] = [x for x in output_words if not (x == 'PAD')] #\n",
        "    predicted = \"\"\n",
        "    for ip in output_words:\n",
        "      if ip == \"EOS\":\n",
        "        break\n",
        "      else:\n",
        "        predicted += ip\n",
        "    #predicted = predicted.ljust(65)\n",
        "    #target = pairr[1].ljust(65)\n",
        "    target = [alpha2index[let] for let in pairr[1]]\n",
        "    predicted = [alpha2index[let] for let in predicted]\n",
        "    N=65\n",
        "    target = (target + N * [''])[:N]\n",
        "    predicted = (predicted + N * [''])[:N]\n",
        "    fscore_eval = f1_score(target, predicted, average='micro')        \n",
        "\n",
        "    accuracy_eval = accuracy_score(target, predicted)\n",
        "    #print(\"Hello\")\n",
        "    return fscore_eval,accuracy_eval\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avXQhPcRsCM5",
        "colab_type": "text"
      },
      "source": [
        "Uncomment for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mR7_s474hlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "tot_fscore_eval = 0\n",
        "tot_accuracy_eval = 0\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting (uncomment and run the following line to begin) \n",
        "for idx,pairr in enumerate(pairs):\n",
        "  #print(idx)\n",
        "  fscore_eval, accuracy_eval = evaluateInput(encoder, decoder, searcher, pairr)\n",
        "  tot_fscore_eval += fscore_eval\n",
        "  tot_accuracy_eval += accuracy_eval\n",
        "\n",
        "  if (idx+1)%20 == 0:\n",
        "    #print(\"________________________________\")\n",
        "    print(\"Iteration : {};     tot_fscore_eval is {:.3f}%       tot_accuracy_eval is {:.3f}%\".format(idx+1, tot_fscore_eval*100/idx, tot_accuracy_eval*5))\n",
        "    #tot_fscore_eval = 0\n",
        "    tot_accuracy_eval = 0\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}